---
layout: default
title: Machine Learning in Action::SVM_SMO_COMPELET
---

<h2>{{ page.title }}</h2>

<hr />

<h2> SVM 概述</h2>

<pre>
	关键词：最优化算法·监督学习
	
	原理 : 将分类问题转化为寻找最佳分割超平面，然后归结为凸优化问题，并利用核技巧将非线性问题转化为线性问题。
	 	二次规划的解决方案：KKT条件。
		快速实现方案：SMO算法。 

</pre>

<hr />

<h2> SMO 完整版代码实现 </h2>

<h3>完整代码 </h3>

<pre>
# 定义优化参数结构体
# 参数列表
# trainFeaturesMat : 训练特征集合 : mat
# trainLabelsMat : 训练标签集合 : mat
# C : 调和系数 : float
# toler : 可接受误差 : float
# m : 样本数量 : int
# alphas : alpha优化系数向量 : mat[float]
# b : b优化截距 ： float
# eCache : 缓存列表，缓存预测误差 ： mat[isCalculted(boolen : 1/0 ),Ei(float)]


class optStruct:
	def __init__(self,trainFeaturesMat,trainLabelsMat,C,toler):
		self.trainFeaturesMat = trainFeaturesMat
		self.trainLabelsMat = trainLabelsMat
		self.C = C
		self.toler = toler
		self.m = shape(trainFeaturesMat[0])
		self.alphas = mat(zeros((self.m,1)))
		self.b = 0.0
		self.eCache = mat(zeros((self.m,2)))



#函数名称:
#	calcEk(oS,k)
#函数功能：
#	计算预测误差
#输入参数：
#	oS : 优化参数结构体变量 
#	k  : 需要计算的样本索引 ： int
#返回参数：
#	返回索引为k的样本的误差

#这个是根据实际改动的，本人认为书上的代码实现存在错误。eCache本来做保存误差缓存之用，当alphas中的任何一项出现变动之后，都必须重新更新
def getEk(oS,k):
	#先检查eCache中是否有效
	if oS.eCache[k,0]:
		return oS.eCache[k,1]
	
	#如果缓存中的数据无效，计算预测值
	fXk = float(multiply(oS.alphas,oS.trainLabelsMat).T * (oS.trainFeaturesMat*(oS.trainFeaturesMat[k,:].T))) - oS.b
	#计算误差
	Ek = fXk - float(oS.trainLabelsMat[k])
	#更新缓存--无效原因是为了保证当更新alpha对儿时才对对应的eCache进行更新。这个地方其实一直存在疑问。原书也没有给出好的解释。还是哥没理解？FK
	#反思×2:
	return Ek



#函数名称：
#	selectSecondAlpha(i,oS,Ei)
#函数功能：
#	选择第二个优化变量
#输入参数：
#	i : 第一个优化变量索引 ： int
#	oS ： 优化参数结构体变量 : optStruct
#	Ei : 第一个优化变量的预测误差 ： float
#输出参数：
#	返回最佳选择的第二个优化变量的索引 : int

def selectSecondAlpha(i,oS,Ei):
	
	maxJ = -1 
	maxDelta = 0.0
	Ej = 0.0

	Ei = getEk(oS,i)
	
	#先从当前有效中进行选择
	validEcacheList = nonzero(oS.eCache[:,0].A)[0]
	
	#选区步长最大
	if (len(validEcccheList)) > 1 :
		for j in validEcacheList :
			if j == i :
				continue
			Ej = getEk(oS,j)
			tempDelta = abs(Ei - Ej)
			if tempDelta > maxDelta:
				maxDelta = tempDelta
				maxJ = j
	
	#第一次，随机选取	
	else:
		j = i
		while(j == i):
			j = int(random.uniform(0,oS.m))
		
		maxJ = j
		Ej = getEk(oS,j)
	return maxJ,Ej

#函数名称：
#	updateEk(oS,k)
#函数功能：
#	当一对alpha更新后，要更新对应的误差缓存
#输入参数：
#	oS : 优化参数结构体 ：optStruct
#	k : 更新第k个alpha对应的误差缓存
#返回参数：
#	void 无

def updataEk(oS,k):
	
	#计算预测值
	fXk = float(multiply(oS.alphas,oS.trainLabesMat).T * (oS.trainFeaturesMat * oS.trainFeaturesMat[k,:])) - oS.b
	#误差计算
	Ek = fXk - oS.trainLabelsMat[k]
	
	oS.eCache[k] = [1,Ek]


#函数名称：
#	innerLoop(i,oS)
#函数功能：
#	内层循环，选定Second Alpha 进行更新
#输入参数：
#	i : First Alpha的索引号 ： int
#	oS : 优化数据结构体 ： optStruct
#输出参数：
#	更新alphas 的个数/2 （pairs数）

def innerLoop(i,oS):
	
	#获取First Alpha 的预测误差
	Ei = getEk(i)
	#判断是否违反KKT条件
	if (( oS.alphas[i] &gt; 0)and( oS.trainLabels[i]*Ei &gt; os.toler)) or (( os.alphas[i] &lt; C)and( os.trainLabels[i])*Ei &lt; -toler):
		#违反,选择第Second Alpha,获取其预测误差
		j,Ej = selectSecondAlpha(i,oS,Ei) 
		
		#alpha pair备份
		alphaIOld = oS.alphas[i].copy()
		alphaJOld = oS.alphas[j].copy()
		
		#确定剪辑区间
		if oS.trainLabelsMat[i] == oS.trainLabelsMat[j]:
			L = max( 0.0 , oS.alphas[i] + oS.alphas[j] - oS.C)
			H = min( oS.C , oS.alphas[i] + oS.alphas[j] )	
		else:
			L = max( 0.0 , oS.alphas[j] - oS.alphas[i] )
			H = min( C , oS.alphas[j] - oS.alphas[i] + C )
		
		# 
		if L == H :
			print "L == H" 
			return 0

		#eta计算 eta = k11 + k22 - 2*k12
		#Kernel(x,y) 内积计算
		Kii = oS.trainFeaturesMat[i,:] * oS.trainFeaturesMat[i,:].T
		Kjj = oS.trainFeaturesMat[j,:] * oS.trainFeaturesMat[j,:].T
		Kij = oS.trainFeaturesMat[i,:] * oS.trainFeaturesMat[j,:].T
		eta = Kii + Kjj - 2*Kij
	
		if eta &lt;= 0 :
			print "eat &lt;= 0"
			return 0
d
		
		#updata alpha[j] 
		oS.alphas[j] += oS.trainLabelsMat[j] * (Ei - Ej) / eta
		
		#对alphas[j]进行剪辑
		if oS.alphas[j] &lt; L :
			oS.alphas[j] = L
		elif oS.alphas[j] &gt; H :
			oS.alphas[j] = H
		
		#观察变动状况
		if abs(oS.alphas[j] - alphaJOld) &lt; 0.0001:
			#改变幅度很小，还原
			oS.alphas[j] = alphaJOld
			oS.alphas[i] = alphaIOld
			print "change little"
			return 0

		
		#update alpha[i]
		oS.alphas[i] += (alphaJOld - oS.alphas[j]) * oS.trainLabels[i] * oS.trainLabels[j]
		
		#update b
		bi = - Ei - oS.trainLabelsMat[i] * Kii * (oS.alphas[i] - alphasIOld) - oS.trainLabelsMat[j] * Kji * (oS.alphas[j] - alphasJOld) + oS.b
		bj = - Ej - oS.trainLabelsMat[i] * Kij * (oS.alpahs[i] - alphasIOld) - oS.trainLabelsMat[j] * Kjj * (oS.alphas[j] - alphasJOld) + oS.b
		
		if (oS.alphas[i] &gt; 0) and (oS.alphas[i] &lt; oS.C):
			oS.b = bi
		elif (oS.alphas[j] &gt; 0) and (oS.alphas[j] &lt; oS.C):
			oS.b = bj
		else :
			oS.b = (bi + bj) / 2
		
		#当更新了alpha_i alpha_j 后，对应的误差缓存也需要更新。
		updateEk(oS,i)
		updateEk(oS.j)
		
		#更新一对				 
		return 1
	else:
		return 0
	
		
	

#函数名称：
#	SVM_SMO_Complete(dataFeatures,dataLabels,C,toler,maxIter)
#函数功能：
#	SMO 实现 SVM 的完整版
#输入参数：
#	dataFeatures : 样本特征集合 
#	dataLabels : 样本标签集合
#	C :  调和参数 ： float
#	toler : 容许误差 ： float
#	maxIter : 最大迭代次数 ： int 
#返回参数：
#	alphas , b : 最优回归系数 ，截距 ： arrray or mat , float

def SVM_SMO_Complete(dataFeatures,dataLabels,C=0.6,toler=0.01,maxIter=100):
	
	
	oS = optStruct(mat(dataFeatures),mat(dataLabels),C,toler) 
	
	#entireSet : 遍历整个样本集合标志位
	entireSet = True # 初始阶段要遍历整个集合 
	
	
	#迭代次数
	iter = 0
	while(  (iter &lt; maxIter) and ((entireSet) or (alphasChangedPairs &gt; 0)):
		#更新alpha对数
		alphasChangedPairs = 0 
		
		#
		if entireSet :
			#遍历整个样本集
			for i in range(oS.m):
				#获取更新的对数
				alphasChangedPairs += innerLoop(i,oS)
		
		else:
			#遍历分隔边界上的点
			nonBoundIs = nonzeros( (oS.alphas.T &gt; 0) and (oS.alphas.T &lt; oS.C) )[0]
			for i in nonBoundIs :
				alphasChangedPairs += innerLoop(i,oS)
		
		
		if entireSet :
			entireSet = False
		
		elif alphasChanged == 0 :
			entireSet = True
		
		iter += 1
	
	return oS.alphas, oS.b

	
------------------------------------------------------------------------------	
</pre>




<hr />
<p> Welcome to contact <a href="mailto:jangwee1@sina.com.cn">me</a>,the friends who like Machine-Learning and Data-Mining.</p>
<p> <a href=freedom.png>:)</a></p>

<p>{{ page.date | date_to_string }}</p>

