---
layout: default
title: Machine Learning in Theory::Naive Bayes
---

<h2>{{ page.title }}</h2>

<hr />



<pre>
	大二的时候学习概率论，理学院的于老师给讲的。当时的课程没怎么好好听，最后也就考了85分左右，大部分的知识都是云里雾里。后来的通信原理，信息论，不少都是跟其息息相关。
	所以现在，只能重新回顾，深入了解一番。

	学过概率论相关课程的人，对贝叶斯绝对不会陌生。几乎每本相关的书上，上来就给扔出一个相关的贝叶斯公式。本人成绩也不好，学得也不是很深入，就记录一些我觉得引人入胜的东西。

	思路由以下几个方面：
	1 为何要统计，为何要概率
	2 预测 模型  
	3 准确度的含义
	4 先验概率 后验概率
	5 过拟合问题
	
	先考虑一个问题。假设设计一个邮件分类器（能够根据邮件内容将一些广告相关的邮件识别为垃圾邮件）。训练集是大量的做好标记<strong>C<sub>i</sub></strong>（这里为二元分类，是否为垃圾邮件）的邮件。
	首先将邮件中词提取出来，获得一个词集向量<strong><i>W</i></strong>。朴素贝叶斯的方式，就是求出使 P(<strong>C<sub>i</sub></strong>|<strong><i>W</i></strong>) 最大的分类<strong>C<sub>i</sub></strong>
	
	下面就用到贝叶斯公式了，接下来的讨论都会围绕这个公式展开：

        -------------------------- 
	|     	     		P( <strong><i>W</i></strong> | <strong><i>C<sub>i</sub></i></strong> )*P( <strong><i>C<sub>i</sub></i></strong> )
	|	P( <strong><i>C<sub>i</sub></i></strong> | <strong><i>W</i></strong> ) = —————
	|           		P( <strong><i>W</i></strong> )   
        --------------------------

	

</pre>
