---
layout: default
title: Machine Learning in Theory::Naive Bayes
---

<h2>{{ page.title }}</h2>

<hr />



<pre>
	大二的时候学习概率论，理学院的于老师给讲的。当时的课程没怎么好好听，最后也就考了85分左右，大部分的知识都是云里雾里。后来的通信原理，信息论，不少都是跟其息息相关。
	所以现在，只能重新回顾，深入了解一番。

	学过概率论相关课程的人，对贝叶斯绝对不会陌生。几乎每本相关的书上，上来就给扔出一个相关的贝叶斯公式。本人成绩也不好，学得也不是很深入，就记录一些我觉得引人入胜的东西。

	思路由以下几个方面：
	1 为何要统计，为何要概率
	2 预测 模型  
	3 准确度的含义
	4 先验概率 后验概率
	5 过拟合问题
	
	先考虑一个问题。假设设计一个邮件分类器（能够根据邮件内容将一些广告相关的邮件识别为垃圾邮件）。训练集是大量的做好标记<strong>C<sub>i</sub></strong>（这里为二元分类，是否为垃圾邮件）的邮件。
	首先将邮件中词提取出来，获得一个词集向量<strong><i>W</i></strong>。朴素贝叶斯的方式，就是求出使 P(<strong>C<sub>i</sub></strong>|<strong><i>W</i></strong>) 最大的分类<strong>C<sub>i</sub></strong>
	
	下面就用到贝叶斯公式了，接下来的讨论都会围绕这个公式展开：

        ---------------------------------------------------------------------------------------------------------------------------------- 
	|     	      	        	P( <strong><i>W </i></strong>| <strong>C<sub>i </sub></strong> )*P( <strong>C<sub>i </sub></strong> )
	|	P( <strong>C<sub>i </sub></strong> | <strong><i>W </i></strong>) = —————
	|			        P( <strong><i>W </i></strong> )   
        ----------------------------------------------------------------------------------------------------------------------------------


	我们先来分析一下各个表达式的含义：
         P( <strong>C<sub>i </sub></strong> | <strong><i>W </i></strong>) ： 在获得观测数据 <strong><i>W </i></strong> 后，推测属于 <strong>C<sub>i </sub></strong> 的概率，属于<strong>后验概率</strong>。
 	 P( <strong><i>W </i></strong>| <strong>C<sub>i </sub></strong> ) : 当邮件属于 <strong>C<sub>i </sub></strong> 时，它能够生成观测数据  <strong><i>W </i></strong> 的概率，属于<strong>似然函数</strong>
	 P( <strong>C<sub>i </sub></strong> ) ： 邮件属于 <strong>C<sub>i </sub></strong> 的概率，属于<strong>先验概率</strong>。
	 P( <strong><i>W </i></strong> ) ： 词集向量<strong><i>W</i></strong> 出现的概率，属于<strong>先验概率</strong>。
	
	 解释一下:
	<strong>先验概率(prior probability):</strong> In Bayesian statistical inference, a prior probability distribution, often called simply the prior, of an uncertain quantity <i> p</i> is the
	probability distribution that would <strong>express one's uncertainty about <i>p</i> before some evidence is taken into account</strong>. For example, <i>p</i> could be the probability 
	distribution for the proportion of voters who will vote for a particular politician in a future election. It is meant to attribute uncertainty, rather than randomness, to the uncertain quantity. 
	The unknown quantity may be a parameter or latent variable.----from wiki
	简单来讲：先验概率，就是不考虑观测数据，就某件事情发生的概率。“先验”并非先于一切经验（毕竟事件本身的成立情况也是由之前的经验和数据统计情况得出的），而是说先于当前的观测数据。在这个邮件分类器的
	设计中，P( <strong>C<sub>i </sub></strong> ) 指给你一封邮件，你去猜它是否属于垃圾邮件。这个概率不考虑邮件本身的内容，也就是观测到的词集向量，凭借给的训练集合的数据计算出的。
	 P( <strong><i>W </i></strong> ) 指的是词集向量出现的概率，也是训练集合的数据统计出的。
        <strong>似然函数(likelihood function)</strong> 在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型中参数的似然性。“似然性”与“概率”的意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”与“概率”又有明确的区分，概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。
	<strong>后验概率：</strong> In Bayesian statistics, the posterior probability of a random event or an uncertain proposition is the conditional probability that is <strong>assigned after the 
	relevant evidence or background is taken into account</strong>. Similarly, the posterior probability distribution is the probability distribution of an unknown quantity, treated as a 
	random variable, conditional on the evidence obtained from an experiment or survey. "Posterior", in this context, means after taking into account the relevant evidence related to the particular 
	case being examined.----from wiki
	简单来讲：后验概率，就是在考虑观测数据下，推测事件发生的发生概率。

	

</pre>
