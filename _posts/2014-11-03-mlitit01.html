---
layout: default
title: Machine Learning in Theory::Naive Bayes
---

<h2>{{ page.title }}</h2>

<hr />

<h3> 概述 </h3>

<pre>
	大二的时候学习概率论，理学院的于老师给讲的。当时的课程没怎么好好听，最后也就考了85分左右，大部分的知识都是云里雾里。后来的通信原理，信息论，不少都是跟其息息相关。
	所以现在，只能重新回顾，深入了解一番。

	学过概率论相关课程的人，对贝叶斯绝对不会陌生。几乎每本相关的书上，上来就给扔出一个相关的贝叶斯公式。本人成绩也不好，学得也不是很深入，就记录一些我觉得引人入胜的东西。

	思路由以下几个方面：
	1 为何要统计，为何要概率
	2 预测 模型  
	3 准确度的含义
	4 先验概率 后验概率
	5 过拟合问题
	:
	先考虑一个问题。假设设计一个邮件分类器（能够根据邮件内容将一些广告相关的邮件识别为垃圾邮件）。训练集是大量的做好标记<strong>C<sub>i</sub></strong>（这里为二元分类，是否为垃圾邮件）的邮件。
	首先将邮件中词提取出来，获得一个词集向量<strong><i>W</i></strong>。朴素贝叶斯的方式，就是求出使 P(<strong>C<sub>i</sub></strong>|<strong><i>W</i></strong>) 最大的分类<strong>C<sub>i</sub></strong>
	
	下面就用到贝叶斯公式了，接下来的讨论都会围绕这个公式展开：

        ---------------------------------------------------------------------------------------------------------------------------------- 
	|     	      	        	P( <strong><i>W </i></strong>| <strong>C<sub>i </sub></strong> )*P( <strong>C<sub>i </sub></strong> )
	|	P( <strong>C<sub>i </sub></strong> | <strong><i>W </i></strong>) = —————
	|			        P( <strong><i>W </i></strong> )   
        ----------------------------------------------------------------------------------------------------------------------------------


	我们先来分析一下各个表达式的含义：
         P( <strong>C<sub>i </sub></strong> | <strong><i>W </i></strong>) ： 在获得观测数据 <strong><i>W </i></strong> 后，推测属于 <strong>C<sub>i </sub></strong> 的概率，属于<strong>后验概率</strong>。
 	 P( <strong><i>W </i></strong>| <strong>C<sub>i </sub></strong> ) : 当邮件属于 <strong>C<sub>i </sub></strong> 时，它能够生成观测数据  <strong><i>W </i></strong> 的概率，属于<strong>似然函数</strong>
	 P( <strong>C<sub>i </sub></strong> ) ： 邮件属于 <strong>C<sub>i </sub></strong> 的概率，属于<strong>先验概率</strong>。
	 P( <strong><i>W </i></strong> ) ： 词集向量<strong><i>W</i></strong> 出现的概率，属于<strong>先验概率</strong>。
	
	 解释一下:
	<strong>先验概率(prior probability):</strong> In Bayesian statistical inference, a prior probability distribution, often called simply the prior, of an uncertain quantity <i> p</i> is the
	probability distribution that would <strong>express one's uncertainty about <i>p</i> before some evidence is taken into account</strong>. For example, <i>p</i> could be the probability 
	distribution for the proportion of voters who will vote for a particular politician in a future election. It is meant to attribute uncertainty, rather than randomness, to the uncertain quantity. 
	The unknown quantity may be a parameter or latent variable.----from wiki
	简单来讲：先验概率，就是不考虑观测数据，就某件事情发生的概率。“先验”并非先于一切经验（毕竟事件本身的成立情况也是由之前的经验和数据统计情况得出的），而是说先于当前的观测数据。在这个邮件分类器的
	设计中，P( <strong>C<sub>i </sub></strong> ) 指给你一封邮件，你去猜它是否属于垃圾邮件。这个概率不考虑邮件本身的内容，也就是观测到的词集向量，凭借给的训练集合的数据计算出的。
	 P( <strong><i>W </i></strong> ) 指的是词集向量出现的概率，也是训练集合的数据统计出的。

        <strong>似然函数(likelihood function)</strong> 在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型中参数的似然性。“似然性”与“概率”的意思相近，都是指某种事件发生的可能性，
	但是在统计学中，“似然性”与“概率”又有明确的区分，概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。
	就是说，在确定邮件的分类（是否是垃圾邮件）的前提下，它能生成某个内容的概率。其实，在这里，我更倾向于将其看作是一个<strong>先验概率</strong>
		

	<strong>后验概率：</strong> In Bayesian statistics, the posterior probability of a random event or an uncertain proposition is the conditional probability that is <strong>assigned after the 
	relevant evidence or background is taken into account</strong>. Similarly, the posterior probability distribution is the probability distribution of an unknown quantity, treated as a 
	random variable, conditional on the evidence obtained from an experiment or survey. "Posterior", in this context, means after taking into account the relevant evidence related to the particular 
	case being examined.----from wiki
	简单来讲：后验概率，就是在考虑观测数据下，推测事件发生的发生概率。P( <strong>C<sub>i </sub></strong> | <strong><i>W </i></strong>) 就是在观测到内容向量的前提下，所属分类的概率。

	将上面的涉及到信息抽象化一点。
	
	对于想研究一个事件或是现象，我们最终想获得的是关于这个事件或现象的模型，分析出它的原因，还原事物的本质。在通信中，就是说，通过接受到的信号，还原出发送的信号。因为原始的发送信号，在信息的传输和表达
	过程中，受到复杂多变的环境影响，导致失真。在概率论的层面上来进行分类时，也就是去验证各种“可能”的可能性大小。在这里，牵扯到两个概念：<strong><i>D </i></strong> (data:观测到的数据) <strong>H<sub>i </sub></strong>(hypothesis:对原型的猜测)，因此贝叶斯可以归结为：
	----------------------------------------------------------------------------------------------------------------------------------
        |                               P( <strong><i>D </i></strong>| <strong>H<sub>i </sub></strong> )*P( <strong>H<sub>i </sub></strong> )
        |       P( <strong>H<sub>i </sub></strong> | <strong><i>D </i></strong>) = —————                                                       ：公式[1]          
        |                               P( <strong><i>D </i></strong> )
        ----------------------------------------------------------------------------------------------------------------------------------
	
	因为P( <strong><i>D </i></strong> ) 对比较的结果不会产生较大影响，上式子可以简化为：
	
	-------------------------------------------------------
	| 	P( <strong>H<sub>i </sub></strong> | <strong><i>D </i></strong>) ∝  P( <strong><i>D </i></strong>| <strong>H<sub>i </sub></strong> )*P( <strong>H<sub>i </sub></strong> )  : 公式[2]
	-------------------------------------------------------
	
	这个式子的意义：
        -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	对于给定的一个观测数据 <strong><i>D </i></strong>，一个猜测 <strong>H<sub>i </sub></strong> 的好坏取决与： <strong> 这个猜测本身独立的可能性大小---先验概率</strong> P( <strong>H<sub>i </sub></strong> ) 和 <strong>这个猜测生成我们观测的数据可能性大小----似然</strong>  P( <strong><i>D </i></strong>| <strong>H<sub>i </sub></strong> ) 的乘积。
	-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	下面的讨论，也将主要围绕公式[2]进行。
</pre>

<hr />

<h3> 讨论 </h3>

<h4>[1]仅用 P( <strong><i>D </i></strong>| <strong>H<sub>i </sub></strong> ) 来判决不够好吗？</h4>

<pre>
	不够好。原因有一下两点：
	(1)  P( <strong><i>D </i></strong>| <strong>H<sub>i </sub></strong> ) 不能提供决策的全部信息。
	在一个输入的纠错系统中，用户输入了‘tlp’，你觉得他是想输入‘top’还是‘tip’呢？因为‘l’与‘i’或‘o’的距离相近，所以 ‘tlp’ 打成 ‘tip’ 或者 ‘top’ 可能性相同（即 P('tlp'|'tip') = P('tlp'|'top') ）.
	(2) 即使一个猜测与观测数据非常符合，也并不说明这个猜测是一个好的猜测。因为有可能这个猜测本身发生的概率非常低。
	给你一组数据[-1,3,7,11]去拟合， 是等差数列有可能，还是-3/11 · X<sup>3</sup> + 9/11 · X<sup>2</sup> + 3/11 这个多项式有可能？（N-点拟合问题）
	在我们的常识（也就是基于生活的统计）,低阶更常见。
	
	以上分析中隐含的哲学是：<strong>观测数据总会有各种各样的误差</strong>
	比如观测误差，如果过分去寻找能够完美解析观测数据的模型，就会导致<strong>overfitting</strong>。一个overfitting的模型试图连误差都去解释（误差是不需要解释的），就显得过犹不及了。
	P( <strong><i>D </i></strong>| <strong>H<sub>i </sub></strong> ) 大不代表你的<strong>H<sub>i </sub></strong> 是个好的猜测。<strong>奥卡姆剃刀</strong>精神就是说：如果两个理论有相似的解释力度，
	优先选择更简单的那个，往往也是更平凡的，更少繁冗的，更常见的。
	
------------------------------------------------------------------------------
</pre>

<h4>[2] overfitting </h4>

<pre>
	overfitting的另一个原因：并非是由于数据的“不精确”（误差导致）,而是因为真实世界对数据的结果产生贡献的因素太多太多，跟噪音不同，这些偏差是另外一些因素集体贡献的结果，不是你的模型所能解释的---
	噪音是不需要解释的---一个现实的模型往往只提取n个与结果相关度很高的，很重要的因素。这个时候，观察的数据会倾向围绕你的有限模型预测结果呈正态分布，于是你的观察数据结果就是这个正态分布的随机取样。
	这个取样很可能受到其余因素的影响偏离你的模型所预测的中心，这个时候便不能贪心的试图通过改变模型来匹配数据，因为那些使结果偏离你的预测的贡献因素并非是你这个有限的模型里面含有的因素所能概括的，
	硬要打肿脸充胖子只能导致不实际的模型。

------------------------------------------------------------------------------

</pre>

<h4>[3] 最大似然</h4>

<pre>
	当出现如下情况：
	（1）对发生的先验概率P( <strong>H<sub>i </sub></strong> )一无所知，我们只能假设每种猜测的先验概率均等。
	（2）根据以往的经验得出，手头的先验概率P( <strong>H<sub>i </sub></strong> )是均等的。
	两种情况都是：
	
		<strong>ArgMax</strong> { 公式[2] } =  <strong>ArgMax</strong>{ P( <strong><i>D </i></strong>| <strong>H<sub>i </sub></strong> ) }
	
	所以这个时候必须依赖于用<strong>最大似然</strong>了。
	
	看下面这个图：


			_/\
		      _/   \_
                     /       \
	            /_________\
		       |   |
                       |   |
		       |   |
		 ______|   |_______
		|      |   |       |
		|      |   |   	   |
	________|______|___|_______|______________
	

	试问： 这棵树后面是一个箱子还是两个箱子？
	
	1个：                                                  2个：
             
                       |   |                                              |   |
                 ______|___|_______                               ________|_ _|________
                |      |   |       |                             |         | | 
                |      |   |       |                             |         | |
        ________|______|___|_______|_____                   _____|_________| |________


		
</pre>




